---
title: "Build a Full-Stack Transcription App with WhisperAPI in 30 Minutes"
description: "Step-by-step tutorial to build a production-ready transcription application using WhisperAPI, React, and Node.js. Complete with code examples and deployment guide."
publishDate: 2025-01-12
tags: ["tutorial", "whisperapi", "react", "nodejs", "full-stack"]
---

Building a transcription application might seem daunting, but with WhisperAPI, you can create a production-ready app in just 30 minutes. In this tutorial, we'll build a full-stack application that accepts audio files, transcribes them using WhisperAPI, and displays the results in a beautiful interface.

## What We're Building

By the end of this tutorial, you'll have:
- A React frontend with drag-and-drop file uploads
- A Node.js/Express backend integrated with WhisperAPI
- Real-time progress tracking during transcription
- A clean UI to display and download transcription results
- A deployable application ready for production

**Prerequisites:**
- Basic knowledge of JavaScript and React
- Node.js 16+ installed
- A WhisperAPI account (sign up at whisperapi.com)
- 30 minutes of your time

The final application will handle multiple audio formats, provide real-time status updates, and deliver accurate transcriptions in seconds.

## Project Setup

Let's start by creating our project structure. We'll use a monorepo approach with separate frontend and backend folders.

```bash
# Create project directory
mkdir transcription-app
cd transcription-app

# Initialize the project
npm init -y

# Create frontend with Vite
npm create vite@latest frontend -- --template react
cd frontend
npm install
cd ..

# Create backend directory
mkdir backend
cd backend
npm init -y
```

Now install the necessary dependencies:

```bash
# Backend dependencies
cd backend
npm install express multer axios dotenv cors

# Frontend dependencies
cd ../frontend
npm install axios react-dropzone
```

Your project structure should look like this:

```
transcription-app/
├── backend/
│   ├── server.js
│   ├── routes/
│   │   └── transcribe.js
│   └── package.json
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── FileUpload.jsx
│   │   │   └── TranscriptionResult.jsx
│   │   ├── App.jsx
│   │   └── main.jsx
│   └── package.json
└── package.json
```

## Backend Setup

Let's build our Express server with WhisperAPI integration. Create `backend/server.js`:

```javascript
const express = require('express');
const multer = require('multer');
const cors = require('cors');
const path = require('path');
require('dotenv').config();

const app = express();
const PORT = process.env.PORT || 3001;

// Middleware
app.use(cors());
app.use(express.json());

// Configure multer for file uploads
const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    cb(null, 'uploads/');
  },
  filename: (req, file, cb) => {
    const uniqueSuffix = Date.now() + '-' + Math.round(Math.random() * 1E9);
    cb(null, file.fieldname + '-' + uniqueSuffix + path.extname(file.originalname));
  }
});

const upload = multer({
  storage: storage,
  limits: { fileSize: 100 * 1024 * 1024 }, // 100MB limit
  fileFilter: (req, file, cb) => {
    const allowedTypes = /mp3|wav|m4a|flac|ogg|webm|mp4/;
    const extname = allowedTypes.test(path.extname(file.originalname).toLowerCase());
    const mimetype = allowedTypes.test(file.mimetype);

    if (mimetype && extname) {
      return cb(null, true);
    } else {
      cb(new Error('Invalid file type. Only audio files are allowed.'));
    }
  }
});

// Create uploads directory
const fs = require('fs');
if (!fs.existsSync('uploads')) {
  fs.mkdirSync('uploads');
}

// Import routes
const transcribeRoute = require('./routes/transcribe');
app.use('/api', transcribeRoute(upload));

// Error handling middleware
app.use((err, req, res, next) => {
  console.error(err.stack);
  res.status(500).json({
    error: err.message || 'Something went wrong!'
  });
});

app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

Now create the transcription route at `backend/routes/transcribe.js`:

```javascript
const express = require('express');
const axios = require('axios');
const FormData = require('form-data');
const fs = require('fs');

module.exports = (upload) => {
  const router = express.Router();
  const WHISPER_API_KEY = process.env.WHISPER_API_KEY;
  const WHISPER_API_URL = 'https://api.whisperapi.com/v1';

  // Upload and transcribe endpoint
  router.post('/transcribe', upload.single('audio'), async (req, res) => {
    try {
      if (!req.file) {
        return res.status(400).json({ error: 'No file uploaded' });
      }

      // Create form data for WhisperAPI
      const formData = new FormData();
      formData.append('file', fs.createReadStream(req.file.path));
      formData.append('model', 'whisper-large-v3');

      // Optional parameters
      if (req.body.language) {
        formData.append('language', req.body.language);
      }
      if (req.body.timestamp_granularities) {
        formData.append('timestamp_granularities', req.body.timestamp_granularities);
      }

      // Send to WhisperAPI
      const response = await axios.post(
        `${WHISPER_API_URL}/transcriptions`,
        formData,
        {
          headers: {
            'Authorization': `Bearer ${WHISPER_API_KEY}`,
            ...formData.getHeaders()
          }
        }
      );

      // Clean up uploaded file
      fs.unlinkSync(req.file.path);

      // Return transcription result
      res.json({
        success: true,
        transcription: response.data.text,
        segments: response.data.segments,
        language: response.data.language,
        duration: response.data.duration
      });

    } catch (error) {
      // Clean up file on error
      if (req.file && fs.existsSync(req.file.path)) {
        fs.unlinkSync(req.file.path);
      }

      console.error('Transcription error:', error.response?.data || error.message);
      res.status(500).json({
        error: error.response?.data?.error || 'Transcription failed',
        details: error.message
      });
    }
  });

  // Health check endpoint
  router.get('/health', (req, res) => {
    res.json({ status: 'ok', timestamp: new Date().toISOString() });
  });

  return router;
};
```

Create a `.env` file in the backend directory:

```
WHISPER_API_KEY=your_api_key_here
PORT=3001
```

## Frontend Implementation

Now let's build our React frontend. Start with the main `App.jsx`:

```javascript
import { useState } from 'react';
import FileUpload from './components/FileUpload';
import TranscriptionResult from './components/TranscriptionResult';
import './App.css';

function App() {
  const [transcription, setTranscription] = useState(null);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState(null);

  const handleTranscriptionComplete = (result) => {
    setTranscription(result);
    setIsLoading(false);
    setError(null);
  };

  const handleTranscriptionStart = () => {
    setIsLoading(true);
    setError(null);
    setTranscription(null);
  };

  const handleError = (errorMessage) => {
    setError(errorMessage);
    setIsLoading(false);
  };

  const handleReset = () => {
    setTranscription(null);
    setError(null);
  };

  return (
    <div className="app">
      <header className="app-header">
        <h1>Audio Transcription App</h1>
        <p>Upload your audio file and get instant transcription</p>
      </header>

      <main className="app-main">
        {error && (
          <div className="error-message">
            <p>{error}</p>
            <button onClick={handleReset}>Try Again</button>
          </div>
        )}

        {!transcription && !isLoading && !error && (
          <FileUpload
            onTranscriptionStart={handleTranscriptionStart}
            onTranscriptionComplete={handleTranscriptionComplete}
            onError={handleError}
          />
        )}

        {isLoading && (
          <div className="loading-state">
            <div className="spinner"></div>
            <p>Transcribing your audio...</p>
            <p className="loading-subtext">This may take a few moments</p>
          </div>
        )}

        {transcription && (
          <TranscriptionResult
            result={transcription}
            onReset={handleReset}
          />
        )}
      </main>

      <footer className="app-footer">
        <p>Powered by WhisperAPI</p>
      </footer>
    </div>
  );
}

export default App;
```

Create the FileUpload component at `frontend/src/components/FileUpload.jsx`:

```javascript
import { useCallback, useState } from 'react';
import { useDropzone } from 'react-dropzone';
import axios from 'axios';

const API_URL = 'http://localhost:3001/api';

function FileUpload({ onTranscriptionStart, onTranscriptionComplete, onError }) {
  const [selectedFile, setSelectedFile] = useState(null);
  const [language, setLanguage] = useState('');

  const onDrop = useCallback((acceptedFiles) => {
    if (acceptedFiles.length > 0) {
      setSelectedFile(acceptedFiles[0]);
    }
  }, []);

  const { getRootProps, getInputProps, isDragActive } = useDropzone({
    onDrop,
    accept: {
      'audio/*': ['.mp3', '.wav', '.m4a', '.flac', '.ogg', '.webm'],
      'video/*': ['.mp4']
    },
    maxFiles: 1,
    maxSize: 100 * 1024 * 1024 // 100MB
  });

  const handleUpload = async () => {
    if (!selectedFile) return;

    onTranscriptionStart();

    const formData = new FormData();
    formData.append('audio', selectedFile);
    if (language) {
      formData.append('language', language);
    }

    try {
      const response = await axios.post(`${API_URL}/transcribe`, formData, {
        headers: {
          'Content-Type': 'multipart/form-data'
        }
      });

      onTranscriptionComplete(response.data);
    } catch (error) {
      onError(error.response?.data?.error || 'Failed to transcribe audio');
    }
  };

  return (
    <div className="file-upload">
      <div {...getRootProps()} className={`dropzone ${isDragActive ? 'active' : ''}`}>
        <input {...getInputProps()} />
        {selectedFile ? (
          <div className="file-info">
            <p>Selected: {selectedFile.name}</p>
            <p className="file-size">
              {(selectedFile.size / 1024 / 1024).toFixed(2)} MB
            </p>
          </div>
        ) : (
          <div className="dropzone-content">
            <svg className="upload-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M7 16a4 4 0 01-.88-7.903A5 5 0 1115.9 6L16 6a5 5 0 011 9.9M15 13l-3-3m0 0l-3 3m3-3v12" />
            </svg>
            <p>Drag and drop your audio file here</p>
            <p className="dropzone-subtext">or click to browse</p>
            <p className="supported-formats">
              Supports: MP3, WAV, M4A, FLAC, OGG, WEBM, MP4
            </p>
          </div>
        )}
      </div>

      {selectedFile && (
        <div className="upload-options">
          <div className="language-selector">
            <label htmlFor="language">Language (optional):</label>
            <select
              id="language"
              value={language}
              onChange={(e) => setLanguage(e.target.value)}
            >
              <option value="">Auto-detect</option>
              <option value="en">English</option>
              <option value="es">Spanish</option>
              <option value="fr">French</option>
              <option value="de">German</option>
              <option value="it">Italian</option>
              <option value="pt">Portuguese</option>
              <option value="zh">Chinese</option>
              <option value="ja">Japanese</option>
              <option value="ko">Korean</option>
            </select>
          </div>

          <div className="upload-actions">
            <button onClick={() => setSelectedFile(null)} className="btn-secondary">
              Cancel
            </button>
            <button onClick={handleUpload} className="btn-primary">
              Transcribe
            </button>
          </div>
        </div>
      )}
    </div>
  );
}

export default FileUpload;
```

Create the TranscriptionResult component at `frontend/src/components/TranscriptionResult.jsx`:

```javascript
function TranscriptionResult({ result, onReset }) {
  const handleCopy = () => {
    navigator.clipboard.writeText(result.transcription);
    alert('Transcription copied to clipboard!');
  };

  const handleDownload = () => {
    const blob = new Blob([result.transcription], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `transcription-${Date.now()}.txt`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
  };

  const formatDuration = (seconds) => {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };

  return (
    <div className="transcription-result">
      <div className="result-header">
        <h2>Transcription Complete</h2>
        <div className="result-meta">
          {result.language && (
            <span className="meta-item">Language: {result.language}</span>
          )}
          {result.duration && (
            <span className="meta-item">Duration: {formatDuration(result.duration)}</span>
          )}
        </div>
      </div>

      <div className="result-content">
        <p>{result.transcription}</p>
      </div>

      {result.segments && result.segments.length > 0 && (
        <details className="segments-section">
          <summary>View Timestamps</summary>
          <div className="segments-list">
            {result.segments.map((segment, index) => (
              <div key={index} className="segment">
                <span className="segment-time">
                  {formatDuration(segment.start)} - {formatDuration(segment.end)}
                </span>
                <span className="segment-text">{segment.text}</span>
              </div>
            ))}
          </div>
        </details>
      )}

      <div className="result-actions">
        <button onClick={handleCopy} className="btn-secondary">
          Copy Text
        </button>
        <button onClick={handleDownload} className="btn-secondary">
          Download TXT
        </button>
        <button onClick={onReset} className="btn-primary">
          New Transcription
        </button>
      </div>
    </div>
  );
}

export default TranscriptionResult;
```

Add styling to `frontend/src/App.css`:

```css
* {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  min-height: 100vh;
}

.app {
  max-width: 800px;
  margin: 0 auto;
  padding: 2rem;
}

.app-header {
  text-align: center;
  color: white;
  margin-bottom: 3rem;
}

.app-header h1 {
  font-size: 2.5rem;
  margin-bottom: 0.5rem;
}

.app-header p {
  font-size: 1.1rem;
  opacity: 0.9;
}

.app-main {
  background: white;
  border-radius: 12px;
  padding: 2rem;
  box-shadow: 0 10px 40px rgba(0, 0, 0, 0.1);
  min-height: 400px;
}

.dropzone {
  border: 3px dashed #cbd5e0;
  border-radius: 8px;
  padding: 3rem 2rem;
  text-align: center;
  cursor: pointer;
  transition: all 0.3s ease;
}

.dropzone:hover,
.dropzone.active {
  border-color: #667eea;
  background: #f7fafc;
}

.upload-icon {
  width: 64px;
  height: 64px;
  margin: 0 auto 1rem;
  color: #667eea;
}

.dropzone-content p {
  font-size: 1.1rem;
  color: #2d3748;
  margin-bottom: 0.5rem;
}

.dropzone-subtext {
  font-size: 0.9rem;
  color: #718096;
}

.supported-formats {
  font-size: 0.85rem;
  color: #a0aec0;
  margin-top: 1rem;
}

.upload-options {
  margin-top: 2rem;
}

.language-selector {
  margin-bottom: 1.5rem;
}

.language-selector label {
  display: block;
  margin-bottom: 0.5rem;
  font-weight: 500;
  color: #2d3748;
}

.language-selector select {
  width: 100%;
  padding: 0.75rem;
  border: 2px solid #e2e8f0;
  border-radius: 6px;
  font-size: 1rem;
}

.upload-actions {
  display: flex;
  gap: 1rem;
  justify-content: flex-end;
}

.btn-primary,
.btn-secondary {
  padding: 0.75rem 1.5rem;
  border-radius: 6px;
  font-size: 1rem;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s ease;
  border: none;
}

.btn-primary {
  background: #667eea;
  color: white;
}

.btn-primary:hover {
  background: #5568d3;
}

.btn-secondary {
  background: #e2e8f0;
  color: #2d3748;
}

.btn-secondary:hover {
  background: #cbd5e0;
}

.loading-state {
  text-align: center;
  padding: 3rem 0;
}

.spinner {
  width: 50px;
  height: 50px;
  margin: 0 auto 1rem;
  border: 4px solid #e2e8f0;
  border-top-color: #667eea;
  border-radius: 50%;
  animation: spin 1s linear infinite;
}

@keyframes spin {
  to { transform: rotate(360deg); }
}

.loading-subtext {
  color: #718096;
  font-size: 0.9rem;
}

.transcription-result {
  animation: fadeIn 0.3s ease;
}

@keyframes fadeIn {
  from { opacity: 0; transform: translateY(10px); }
  to { opacity: 1; transform: translateY(0); }
}

.result-header {
  margin-bottom: 1.5rem;
}

.result-header h2 {
  color: #2d3748;
  margin-bottom: 0.5rem;
}

.result-meta {
  display: flex;
  gap: 1rem;
  color: #718096;
  font-size: 0.9rem;
}

.result-content {
  background: #f7fafc;
  padding: 1.5rem;
  border-radius: 8px;
  margin-bottom: 1.5rem;
  line-height: 1.6;
  color: #2d3748;
}

.segments-section {
  margin-bottom: 1.5rem;
}

.segments-section summary {
  cursor: pointer;
  font-weight: 500;
  color: #667eea;
  margin-bottom: 1rem;
}

.segments-list {
  max-height: 300px;
  overflow-y: auto;
}

.segment {
  display: flex;
  gap: 1rem;
  padding: 0.75rem;
  border-bottom: 1px solid #e2e8f0;
}

.segment-time {
  flex-shrink: 0;
  color: #718096;
  font-family: monospace;
  font-size: 0.85rem;
}

.result-actions {
  display: flex;
  gap: 1rem;
  justify-content: flex-end;
}

.error-message {
  text-align: center;
  padding: 2rem;
  color: #e53e3e;
}

.app-footer {
  text-align: center;
  color: white;
  margin-top: 2rem;
  opacity: 0.8;
}
```

## Running the Application

Start both the backend and frontend:

```bash
# Terminal 1 - Backend
cd backend
node server.js

# Terminal 2 - Frontend
cd frontend
npm run dev
```

Visit `http://localhost:5173` to see your app in action!

## Deployment

Deploy your application to production in minutes.

**Backend (Railway):**

```bash
cd backend
railway login
railway init
railway up
```

Add your environment variables in the Railway dashboard.

**Frontend (Vercel):**

```bash
cd frontend
npm install -g vercel
vercel
```

Update the API_URL in your frontend to point to your deployed backend.

## Next Steps

Enhance your application with:
- **User authentication** for multi-user support
- **Database integration** to store transcription history
- **Batch processing** for multiple files
- **Advanced features** like speaker diarization and custom vocabulary
- **Payment integration** for a SaaS product

You now have a fully functional transcription app! The combination of React's interactive UI and WhisperAPI's powerful transcription engine creates a seamless user experience.

**Resources:**
- [WhisperAPI Documentation](https://whisperapi.com/docs)
- [Full source code on GitHub](https://github.com/whisperapi/transcription-app-tutorial)
- [Live demo](https://transcription-app-demo.vercel.app)

Start building amazing voice-powered applications today with WhisperAPI!
